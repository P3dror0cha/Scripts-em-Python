from Bio import Entrez
import pandas as pd
import xml.etree.ElementTree as ET
import xmltodict
import time
import click
import json
import requests
import re

# Configurações globais
Entrez.email = 'pedro.rodrigues.rocha@usp.br'
Entrez.api_key = '94372c1e44aa8df940f2e7b790a761a25908'
DEFAULT_QUERY = 'metagenome AND aquatic'

def search_bioprojects(term, max_results):
    """Realiza pesquisa inicial de bioprojetos no NCBI"""
    print ("Iniciando a função search_bioprojects -(1)")
    print(f"Pesquisando bioprojetos com termo: {term}")
    handle = Entrez.esearch(db='bioproject', term=term, retmax=max_results)
    results = Entrez.read(handle)
    handle.close()
    print(f"Foram encontrados {results['Count']} resultados.")
    print("Fim da função search_bioprojects -(1)")
    return results['IdList']

def fetch_bioproject_data(id_list):
    """Obtém dados detalhados de cada bioprojeto"""
    print("Início da função fetch_bioproject_data -(2)")
    print(f"Obtendo dados para {len(id_list)} bioprojetos...")
    
    ids = []
    accessions = []
    organisms = []
    titles = []
    submissions = []
    
    for i, id_number in enumerate(id_list, 1):
        try:
            handle = Entrez.efetch(db='bioproject', id=id_number, rettype='xml', retmode='xml')
            print(f"Processando bioprojeto {i}/{len(id_list)} - ID: {id_number}")
            time.sleep(0.01)
            
            xml_data = handle.read().decode('utf-8')
            handle.close()
            
            data_dict = xmltodict.parse(xml_data)
            doc_summary = data_dict['RecordSet']['DocumentSummary']
            
            ids.append(doc_summary['@uid'])
            accessions.append(doc_summary['Project']['ProjectID']['ArchiveID']['@accession'])
            titles.append(doc_summary['Project']['ProjectDescr']['Title'])
            submissions.append(doc_summary['Submission']['@submitted'])
            
            try:
                organisms.append(doc_summary['Project']['ProjectType']['ProjectTypeSubmission']['Target']['Organism']['OrganismName'])
            except KeyError:
                organisms.append('NA')
                
        except Exception as e:
            print(f"Erro ao processar bioprojeto {id_number}: {e}")
            continue
    
    df = pd.DataFrame({
        'id': ids,
        'bioproject_id': accessions,
        'organism_name': organisms,
        'title': titles,
        'submission': submissions
    })
    
    print("\nDataFrame resultante da função fetch_bioproject_data:")
    print(df.head(25))  
    print(f"\nShape do DataFrame: {df.shape}")
    print("\nTipos de dados:")
    print(df.dtypes)
    print("Fim da função fetch_bioproject_data -(2)")
    return df


def link_bioproject_to_biosample(bioproject_ids):
    """Obtém IDs de biosample relacionados aos bioprojetos"""
    print("Início da função link_bioproject_to_biosample -(3)")
    print("Buscando biosamples relacionados aos bioprojetos...")
    ids_str = ",".join(bioproject_ids)
    
    print("Os ids de bioproject consultados são:")
    print(ids_str)

    response = requests.get(
        "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi",
        params={
            "dbfrom": "bioproject",
            "db": "biosample",
            "id": ids_str,
            "retmode": "json"
        }
    )
    
    data = response.json()
    biosample_ids = []
    
    try:
        for linkset in data["linksets"]:
            for linksetdb in linkset.get("linksetdbs", []):
                if linksetdb["dbto"] == "biosample":
                    biosample_ids.extend(linksetdb["links"])
    except KeyError:
        print("Erro ao processar links entre bioprojetos e biosamples")
    
    print(f"Os ids de biosample encontrados foram: {biosample_ids}")
    print(f"Encontrados {len(biosample_ids)} biosamples relacionados")
    print("Fim da função link_bioproject_to_biosample -(3)")
    return biosample_ids

def fetch_biosample_data(biosample_ids):
    """Obtém dados detalhados de cada biosample"""
    print("Início da função fetch_biosample_data -(4)")
    print(f"Obtendo dados para {len(biosample_ids)} biosamples...")
    
    data = {
        'id': [], 'biosample_id': [], 'SRA_id': [], 'taxonomy_name': [],
        'owner': [], 'broad_scale_enviromental': [], 'geo_loc_name': [],
        'lat_and_lon': [], 'bioproject_id': []
    }
    
    for i, id_number in enumerate(biosample_ids, 1):
        # Possível problema com o id number. Necessáio ver a lista!!!
        # TEM QUE PESQUISAR COM A COLUNA "id". A coluna "biosample_id" leva a resultados errados!!!
        try:
            handle = Entrez.efetch(db='biosample', id=id_number, rettype='xml', retmode='xml')
            print(f"Processando biosample {i}/{len(biosample_ids)} - ID: {id_number}")
            time.sleep(0.01)
            
            xml_data = handle.read().decode('utf-8')
            handle.close()
            
            data_dict = xmltodict.parse(xml_data)
            root = ET.fromstring(xml_data)
            biosample = root.find(".//BioSample")
            
            data['id'].append(data_dict['BioSampleSet']['BioSample']['@id'])
            data['biosample_id'].append(data_dict['BioSampleSet']['BioSample']['@accession'])
            
            sra_element = biosample.find(".//Id[@db='SRA']") if biosample else None
            data['SRA_id'].append(sra_element.text if sra_element else "NA")
            
            # Taxonomia:
            try:
                data['taxonomy_name'].append(data_dict['BioSampleSet']['BioSample']['Description']['Organism']['@taxonomy_name'])
            except:
                data['taxonomy_name'].append('NA')
            
            # Autor:
            try:
                owner = data_dict['BioSampleSet']['BioSample']['Owner']['Name']
                data['owner'].append(owner['#text'] if '#text' in owner else owner)
            except:
                data['owner'].append('NA')
            
            # Atributos:
            attributes = data_dict['BioSampleSet']['BioSample'].get('Attributes', {}).get('Attribute', [])
            if not isinstance(attributes, list):
                attributes = [attributes]
                
            attr_data = {
                'isolation_source': 'NA',
                'geo_loc_name': 'NA',
                'lat_lon': 'NA'
            }
            
            for attr in attributes:
                name = attr['@attribute_name']
                if name in attr_data:
                    attr_data[name] = attr['#text']
            
            data['broad_scale_enviromental'].append(attr_data['isolation_source'])
            data['geo_loc_name'].append(attr_data['geo_loc_name'])
            data['lat_and_lon'].append(attr_data['lat_lon'])
            
            # Bioproject link:
            try:
                data['bioproject_id'].append(data_dict['BioSampleSet']['BioSample']['Links']['Link']['@label'])
            except:
                data['bioproject_id'].append('NA')
                
        except Exception as e:
            print(f"Erro ao processar biosample {id_number}: {e}")
            continue
            
    df_biosample = pd.DataFrame(data).fillna("Vazio")

    print("\nResumo do DataFrame:")
    print(f"Total de registros: {len(df_biosample)}")
    print("\nPrimeiras linhas:")
    print(df_biosample.head(25))
    print("\nValores únicos por coluna:")
    for col in df_biosample.columns:
        print(f"{col}: {df_biosample[col].nunique()} valores únicos")
    

    # Fazendo o download do dataframe de biosample:
    print("Fazendo o download do dataframe de biosample:")
    df_biosample.to_csv("df_biosample.csv", index=False)
    print("Dataframe de Biosample baixado!")
    print("Finalizando a função fetch_biosample_data - (4)")
    return df_biosample

import requests
import pandas as pd
import time
import json
from urllib.parse import quote

def fetch_ena_graphql(biosample_id, max_retries=3):
    """Busca dados no ENA usando GraphQL com tratamento robusto de erros"""
    query = """
    query ($accession: String!) {
      runs(accession: $accession) {
        run_accession
        instrument_platform
        library_layout
        sample {
          sample_accession
        }
      }
    }
    """
    
    variables = {"accession": biosample_id}
    
    headers = {
        "Content-Type": "application/json",
        "Accept": "application/json"
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.post(
                "https://www.ebi.ac.uk/ena/graphql/",
                json={"query": query, "variables": variables},
                headers=headers,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                if "errors" in data:
                    print(f"Erro GraphQL para {biosample_id}: {data['errors']}")
                    return None
                return data.get("data", {}).get("runs", [])
            else:
                print(f"Tentativa {attempt + 1} - Status {response.status_code}")
                time.sleep(2 ** attempt)  # Backoff exponencial
                
        except Exception as e:
            print(f"Tentativa {attempt + 1} - Exceção: {str(e)}")
            time.sleep(5)
    
    print(f"Falha após {max_retries} tentativas para {biosample_id}")
    return None

def fetch_sra_data(df_biosample, email="pedro.rodrigues.rocha@usp.br"):
    """Versão aprimorada com fallback para múltiplas APIs"""
    results = []
    
    for biosample_id in df_biosample["biosample_id"]:
        try:
            print(f"\nProcessando: {biosample_id}")
            
            if biosample_id.startswith("SAMD"):
                results.append({
                    "BioSample": biosample_id,
                    "SRA_ID": "ACESSO RESTRITO (dbGaP)",
                    "Instrumento": "Requer autorização",
                    "Layout": "N/A",
                    "URL": "https://www.ncbi.nlm.nih.gov/gap",
                    "Fonte": "dbGaP"
                })
                continue
                
            # Primeiro tentamos GraphQL
            ena_data = fetch_ena_graphql(biosample_id)
            
            if ena_data:
                for run in ena_data:
                    results.append({
                        "BioSample": biosample_id,
                        "SRA_ID": run.get("run_accession", "N/A"),
                        "Instrumento": run.get("instrument_platform", "N/A"),
                        "Layout": run.get("library_layout", "N/A"),
                        "URL": f"https://www.ebi.ac.uk/ena/browser/view/{run.get('run_accession', '')}",
                        "Fonte": "ENA GraphQL"
                    })
                continue
            
            # Fallback para API REST se GraphQL falhar
            termo_busca = quote(f'biosample_accession={biosample_id}')
            url = f"https://www.ebi.ac.uk/ena/portal/api/filereport?accession={biosample_id}&result=read_run&fields=run_accession,instrument_platform,library_layout&format=json"
            
            try:
                response = requests.get(url, timeout=30)
                if response.status_code == 200:
                    data = response.json()
                    if data:
                        for item in data:
                            results.append({
                                "BioSample": biosample_id,
                                "SRA_ID": item.get('run_accession', 'N/A'),
                                "Instrumento": item.get('instrument_platform', 'N/A'),
                                "Layout": item.get('library_layout', 'N/A'),
                                "URL": f"https://www.ebi.ac.uk/ena/browser/view/{item.get('run_accession', '')}",
                                "Fonte": "ENA REST"
                            })
                        continue
                
                print(f"Nenhum dado encontrado via APIs ENA para {biosample_id}")
                results.append({
                    "BioSample": biosample_id,
                    "SRA_ID": "NÃO ENCONTRADO",
                    "Instrumento": "N/A",
                    "Layout": "N/A",
                    "URL": "N/A",
                    "Fonte": "Nenhuma"
                })
                
            except Exception as e:
                print(f"Erro no fallback REST: {str(e)}")
                results.append({
                    "BioSample": biosample_id,
                    "SRA_ID": f"ERRO: {str(e)}",
                    "Instrumento": "N/A",
                    "Layout": "N/A",
                    "URL": "N/A",
                    "Fonte": "Erro"
                })
            
            time.sleep(1)
            
        except Exception as e:
            print(f"Erro geral com {biosample_id}: {str(e)}")
            results.append({
                "BioSample": biosample_id,
                "SRA_ID": f"ERRO GERAL: {str(e)}",
                "Instrumento": "N/A",
                "Layout": "N/A",
                "URL": "N/A",
                "Fonte": "Erro"
            })
    
    return pd.DataFrame(results)

def filter_data(df, instrument_terms, strategy_terms, location_terms):
    """Filtra os dados conforme critérios especificados"""
    print("Iniciando a função filter_data -(6)")
    print("Filtrando dados...")
    
    # Filtra por instrumento
    instrument_pattern = '|'.join(instrument_terms)
    filtered = df[df['library_instrument'].str.contains(instrument_pattern, case=False, na=False)]
    
    # Filtra por estratégia
    strategy_pattern = '|'.join(strategy_terms)
    filtered = filtered[filtered['library_strategy'].str.contains(strategy_pattern, case=False, na=False)]
    
    # Filtra por localização
    location_pattern = '|'.join(location_terms)
    filtered = filtered[filtered['geo_loc_name'].str.contains(location_pattern, case=False, na=False)]
    
    print(f"Dados filtrados: {filtered.shape[0]} amostras restantes")
    print("A forma das primeiras linhas do df filtrado é:")
    print(filtered.head(25))
    print("Fim da função filter_data -(6)")
    return filtered

def save_and_summarize(df, filename):
    """Salva os dados e exibe um resumo"""
    print("Início da função save_and_summarize -(7)")
    print(f"Salvando dados em {filename}")
    df.to_csv(filename, index=False)
    
    print("\nResumo dos dados:")
    print("\nContagem por instrumento:")
    print(df['library_instrument'].value_counts())
    
    print("\nContagem por estratégia:")
    print(df['library_strategy'].value_counts())
    
    print("\nContagem por localização:")
    print(df['geo_loc_name'].value_counts())
    
    print("\nContagem por projeto:")
    print(df['bioproject_id'].value_counts())
    print("Fim da função save_and_summarize -(7)")

@click.command()
@click.option('--termo_da_pesquisa', default=DEFAULT_QUERY, help="Termo de pesquisa no NCBI.")
@click.option('--num_resultados', default=15, type=int, help="Número máximo de resultados.")
@click.option('--termo_estrategia', default="wgs", help="Tipo de estratégia de sequenciamento.")
@click.option('--termo_instrumento', default="metagenomic,genomic", help="Tipos de instrumentos aceitos.")
@click.option('--termo_localizacao', default="brazil", help="Localização geográfica das amostras.")
def main(termo_da_pesquisa, num_resultados, termo_estrategia, termo_instrumento, termo_localizacao):
    """Pipeline principal para busca e filtragem de dados do NCBI"""
    
    # Processa parâmetros
    instrumentos = [x.strip() for x in termo_instrumento.split(',')]
    localizacoes = [x.strip() for x in termo_localizacao.split(',')]
    
    # 1. Busca bioprojetos
    bioproject_ids = search_bioprojects(termo_da_pesquisa, num_resultados)
    
    # 2. Obtém dados dos bioprojetos
    bioproject_df = fetch_bioproject_data(bioproject_ids)
    
    # 3. Encontra biosamples relacionados
    biosample_ids = link_bioproject_to_biosample(bioproject_ids)
    
    # 4. Obtém dados dos biosamples
    df_biosample = fetch_biosample_data(biosample_ids)
    
    # 5. Combina dados de bioprojeto e biosample
    merged_df = pd.merge(df_biosample, bioproject_df, on='bioproject_id', how='inner')
    
    # 6. Obtém dados SRA
    sra_ids = merged_df['SRA_id'].tolist()
    sra_ids = [re.sub(r"^(SRS|DRS|DRR|SRR)", "", x) for x in sra_ids if x != 'NA']
    sra_df = fetch_sra_data(df_biosample, email="pedro.rodrigues.rocha@usp.br")
    
    # 7. Combina todos os dados
    final_df = pd.merge(sra_df, merged_df, on='biosample_id', how='inner')
    
    # 8. Filtra os dados
    filtered_df = filter_data(final_df, instrumentos, [termo_estrategia], localizacoes)
    
    # 9. Salva e exibe resumo
    save_and_summarize(filtered_df, 'lista_ncbi_filtrada.csv')

if __name__ == "__main__":
    main()
