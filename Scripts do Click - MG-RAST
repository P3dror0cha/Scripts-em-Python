import pandas as pd
import requests
import click

@click.command()
@click.option("--url", default='https://api.mg-rast.org/search?country=Brazil&limit=100', type=str, help="url para a pesquisa dos dados no MG-RAST. O default é: 'https://api.mg-rast.org/search?country=Brazil&limit=100' ")
@click.option("--requisitions", default=5, type=int, help="número de vezes que a API será consultada. O default é 15.")
@click.option("--biome", default="marine|mangrove|aquatic|freshwater|coral|ocean|estuarine|reef|river|oceanic|sea|lake", type=str, help="Seleciona apenas as linhas que contenham as palavras apresentadas.")
@click.option("--sequence_type", default="WGS", type=str, help="Seleciona o tipo de sequência que será escolhida pelo código.")

def mgrast_download_e_filtragem(url, requisitions, biome, sequence_type):
    x = 0
    resultados_totais = []
    while url:
        try:
            resposta = requests.get(url, verify=False)

            if resposta.status_code == 200:
                data = resposta.json()
                data_list = data.get('data', [])
                
                if data_list:
                    resultados_totais.extend(data_list)  
                    click.echo(message=f"Adicionadas {len(data_list)} linhas. Total até agora: {len(resultados_totais)}")
                else:
                    click.echo("Nenhum dado retornado na página atual.")
                    break
                
                x += 1
                print(x)
                
                # Atualização da URL para as próximas páginas
                url = data.get('next', None)
                if requisitions < x:  # Limite de requisições (pode ser alterado conforme necessidade)
                    break
                if url:
                    click.echo(message=f"Avançando para a próxima página: {url}")
                else:
                    click.echo(message="Não há mais páginas para processar.")
                    break
            else:
                click.echo(message=f"Erro ao acessar a API: {resposta.status_code}")
                break

        except requests.exceptions.RequestException as e:
            print(f"Erro na requisição: {e}")
            break

        df_mgrast = pd.DataFrame(resultados_totais)
        # lista_mgrast = df_mgrast.to_csv("lista_mgrast.csv")
        
    print('Esses são os resultados do download:')

    print('Início da filtargem do dataframe:')
    # Filtro 2 = Biomas aquáticos (parâmetro "biome")
    df_limpo = df_mgrast[df_mgrast['biome'].str.contains(biome, case=False, na=False)]
    print('Filtro 1: Bioma das amostras. Os valores da coluna biome são:')
    print(df_limpo['biome'].value_counts())
    print('Somatório das linhas:')
    print(df_limpo['biome'].value_counts().sum())
    # Ao final o df_limpo contem todos os metagenomas BRASILEIROS (filtrado pela API), e AQUÁTICOS (Filtrado pelo filtro 1)
    
    # Filtro 3 = Qualidade das amostras (Sem parâmetros)
    df_limpo = df_limpo.dropna(subset=['drisee_score_raw']) # Retira linhas onde não há valor de drisee_score.
    df_limpo = df_limpo[df_limpo['drisee_score_raw'] != 0] # Retira linhas onde o valor de drisee_score é igual a zero.
    df_limpo = df_limpo[df_limpo['drisee_score_raw'] <= 5.0] # Estipula que o valor máximo do drisee score é de 5.0. Quanto menor o valor, menos erros a sequência tem.
    with pd.option_context('display.max_rows', None):
        print('Filtro 2: Retirando amostras sem qualidade ou com qualidade baixa. Os valores da coluna drisee_score_raw são:')
        print(df_limpo['drisee_score_raw'].agg(['sum', 'mean', 'min', 'max', 'std', 'var']))
        print('Somatório das linhas após análise da qualidade (filtro 2):')
        print(df_limpo.value_counts('drisee_score_raw', dropna=False).sum())
        
    # Filtro 4 = Filtragem de acordo com o tipo de sequência - Sequência preferencialmente selecionada: Whole Genome Sequence (WGS) (parâmetro "sequence_type")
    print('Separando o dataframe nos grupos Whole Metagenome Sequence (WGS), Amplicon e Mitochondrial (MT). Grupos referentes ao tipo de sequência:')
    df_limpo_WGS = df_limpo[df_limpo['sequence_type'] == 'WGS']
    df_limpo_amplicon = df_limpo[df_limpo['sequence_type'] == 'Amplicon']
    df_limpo_MT = df_limpo[df_limpo['sequence_type'] == 'MT']
    
    # Seleção do tipo de sequencias de interesse do usuário:
    if sequence_type == "WGS":
        print('Dataframe WGS selecionado. O shape do df é:')
        df_limpo_sequence_type =df_limpo_WGS
        print(df_limpo_WGS.shape)
        print(df_limpo_WGS)
    elif sequence_type == "Amplicon":
        print('Dataframe Amplicon selecionado. O shape do df é:')
        df_limpo_sequence_type =df_limpo_amplicon
        print(df_limpo_amplicon.shape)
        print(df_limpo_amplicon)
    elif sequence_type == "MT":
        print('Dataframe MT selecionado. O shape do df é:')
        df_limpo_sequence_type =df_limpo_MT
        print(df_limpo_MT.shape)
        print(df_limpo_MT)
        
    print('Separando os grupos "Normal" e "Wastewater":')
    df_limpo_wastewater = df_limpo_sequence_type[df_limpo_sequence_type['env_package_type'].str.contains('wastewater|sludge', case=False, na=False)]
    df_limpo_normal = df_limpo_sequence_type[~df_limpo_sequence_type['env_package_type'].str.contains('wastewater|sludge', case=False, na=False)]

    with pd.option_context('display.max_rows', None):
        print('Somatório das linhas df_wastewater:')
        print(df_limpo_wastewater.shape)
        print('Todos os resultados do df wastewater:')
        print(df_limpo_wastewater.value_counts('biome', dropna=False))
        print(('Somatório das linhas df_normal:'))
        print(df_limpo_normal.shape)
        print('Todos os resultados do df normal:')
        print(df_limpo_normal.value_counts('biome', dropna=False))
        
    # Filtro 5 = Filtragem a partir do tipo de material/água que a amostra faz parte. Para isto, utilizaremos a coluna: 'biome'.
    df_coral_reef = df_limpo_normal[df_limpo_normal['biome'].str.contains('coral', case=False, na=False) & df_limpo_normal['biome'].str.contains('coral', case=False, na=False)]
    print('Tabela com amostras de coral:')
    print(df_coral_reef.shape)
    # df_coral_reef.to_csv('df_coral_reef.csv', index=False)

    df_ocean_biome = df_limpo_normal[df_limpo_normal['biome'].str.contains('ocean', case=False, na=False)]
    print('Tabela com amostras oceânicas:')
    print(df_ocean_biome.shape)
    # df_ocean_biome.to_csv('df_ocean_biome.csv', index=False)

    df_freshwater_biome = df_limpo_normal[df_limpo_normal['biome'].str.contains('freshwater', case=False, na=False)]
    print('Tabela com amostras de água doce:')
    print(df_freshwater_biome.shape)
    # df_freshwater_biome.to_csv('df_freshwater_biome.csv', index=False)

    df_mangrove_biome = df_limpo_normal[df_limpo_normal['biome'].str.contains('mangrove', case=False, na=False)]
    print('Tabela com amostras de mangue:')
    print(df_mangrove_biome.shape)
    # df_mangrove_biome.to_csv('df_mangrove_biome.csv', index=False)
        
    print("Fazendo o download da tabela do MG-RAST:")
    #df_limpo_sequence_type.to_csv('tabela_completa_mgrast.csv')  
    print(df_limpo_sequence_type)
    print(df_limpo_sequence_type.shape)
    print(df_limpo_sequence_type.columns)
        
    # Fazendo uma lista com base na coluna de IDs do DataFrame:
    lista_de_ids_normal = df_limpo_normal['metagenome_id'].tolist()
    lista_de_ids_wastewater = df_limpo_wastewater['metagenome_id'].tolist()
    lista_de_ids_coralreef = df_coral_reef['metagenome_id'].tolist()
    lista_de_ids_ocean = df_ocean_biome['metagenome_id'].tolist()
    lista_de_ids_freshwater = df_freshwater_biome['metagenome_id'].tolist()
    lista_de_ids_mangrove = df_mangrove_biome['metagenome_id'].tolist()
    lista_de_ids_totais = df_limpo_sequence_type['metagenome_id'].tolist()
    print(len(lista_de_ids_wastewater))
    print('Código mg-rast.py concluído!')
       
    import csv 
    dados_brutos = {} 
    for sample_id in lista_de_ids_totais:
        url = f"https://api.mg-rast.org/1/download/{sample_id}"
        response = requests.get(url, verify=False)

        if response.status_code == 200:
            resposta_json = response.json()  # Converte a resposta para JSON
            print(f"Resposta da API para {sample_id}: {resposta_json}")  # Debug

            # Se houver 'data' na resposta, percorre e extrai as URLs
            urls = []
            if 'data' in resposta_json:
                for item in resposta_json['data']:
                    if 'url' in item:  # Verifica se a chave 'url' está presente
                        urls.append(item['url'])  # Adiciona a URL à lista

            if urls:
                dados_brutos[sample_id] = urls  # Armazena as URLs associadas ao ID
                print(f"URLs do sample {sample_id} salvas com sucesso!")
            else:
                dados_brutos[sample_id] = "Sem URLs disponíveis"
        else:
            dados_brutos[sample_id] = f"Erro: {response.status_code}"  # Armazena o erro

    # Exibindo os resultados (ID -> lista de URLs)
    print(dados_brutos)
    
     # Salvando no CSV
    csv_filename = "url_brutas_mgrast.csv"

    with open(csv_filename, "w", newline="") as csvfile:
        fieldnames = ["sample_id", "url"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()  # Escreve o cabeçalho
        
        for sample_id, urls in dados_brutos.items():
            if urls:  # Apenas escreve se houver URLs
                for url in urls:
                    writer.writerow({"sample_id": sample_id, "url": url})
            else:
                writer.writerow({"sample_id": sample_id, "url": "Sem URLs disponíveis"})

    print(f"Dados salvos no arquivo '{csv_filename}'.")

    # Carregar o CSV como DataFrame e exibir
    df = pd.read_csv(csv_filename)
    print(df)
    
if __name__ == "__main__":
    mgrast_download_e_filtragem()

